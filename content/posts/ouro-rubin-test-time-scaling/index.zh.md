---
date: '2026-01-07T11:20:56Z'
title: '从循环模型 Ouro 到 NVIDIA Rubin：推理时代的殊途同归'
slug: 'ouro-rubin-test-time-scaling'
categories: ['AI']
tags: ['test-time-scaling', 'looped-models', 'nvidia', 'inference']
isCJKLanguage: true
---

过去几年，AI 变强的故事讲得很简单：砸更多训练算力、喂更多数据、把模型做得更大。可以把它理解为多读书、多刷题，靠训练阶段把能力压进参数里。可最近一段时间，风向可能有所变化：同样一个模型，不一定要继续变大，只要在遇到难题时多想一会儿，结果就能明显变好。

这件事在行业里有个更正式的名字：Test-time Scaling（推理时扩展）。它背后的直觉也很朴素：训练像上学，推理像考试。以前我们只卷平时多努力，现在开始卷考场多想 30 秒。问题也随之变得更现实：如果每次都多想一会儿，那成本谁来付？怎么付得起？

我最近看到两个方向的进展，恰好把这件事解释得特别清楚：一个是算法侧的循环模型（Looped Language Models / Ouro），另一个是系统与硬件侧的 NVIDIA Rubin。在我看来，它们其实在解决同一个问题的上下两层：前者在教大脑怎么更会想，后者在把更会想变成可以量产、可以商用的基础设施。

## 一、循环模型：神经元没变，但突触回路更强了

如果要简单理解循环模型的独特之处，我更喜欢用人脑做类比：循环模型并没有让神经元变多（参数量没变），但它让突触连接、回路结构变得更强，所以更容易联想、更容易推理。这不是文学化的自嗨，它和论文里一个关键结论几乎一一对应：循环并不会显著增加模型的知识容量，但会显著增强模型使用知识的能力。

具体怎么做到的？LoopLM 的核心设计很简单：把一组 Transformer 层的参数共享起来，在一次前向传播里反复循环使用。可以把它理解为同一套思考模块重复运转多轮，每一轮都在更新隐藏状态，让内部表示逐步变得更到位。这带来两个直接好处：第一，它把计算深度从参数规模里解耦出来；第二，它不是靠生成更长的输出 token 来显式思考，而是让思考发生在 latent space（隐藏状态）里，避免上下文越写越长的膨胀。

在使用层面，循环模型通常会配套一种按需路由的策略：简单问题少循环，难题多循环。这不是拍脑袋规则，而是模型学出来的。论文里提到 learned early-exit（自适应早停）机制，并把 Q-exit 作为部署时可调的阈值，用同一个模型在不同计算预算下做取舍，而且不需要重训。对我来说，它最像人类考试的地方也在这里：不是每道题都写 20 行草稿，而是难题多推敲两遍，简单题看一眼就过。

这里插播一小段私货。我想到一部动漫《异度侵入 ID:INVADED》，男主进入一种心理装置后会变成一个记忆全无的无情破案机器。它很形象地展示了同样的环境里，脑回路强的人（或者说推理回路更顺的人）确实更容易把问题解出来。

## 二、训练期循环 + 推理期循环：缺一不可

看完循环模型的介绍后，我脑子里自然冒出一个问题：那把普通 Transformer 也拿来循环跑几遍，是不是也行？跟 AI 讨论后的结论是：可能有提升，但很难达到循环训练出来的模型的效果。

原因在于，循环推理要想稳定有效，训练时必须植入一些基础设定，比如更深的循环应该带来更好的预测，但该停的时候也要停。这些都是训练目标和训练过程塑造出来的能力。循环模型在预训练阶段就把多轮迭代当作常态，而且往往会在不同循环步上都计算损失，让模型学会每多想一轮，答案分布就更接近正确。论文甚至把这个性质写得很明确：训练目标需要保留 deeper-is-better，让 next-token loss 在期望意义上随循环步数单调改善，于是整个系统变成一种 anytime algorithm——你可以从中间某一步开始输出，同时后面的步骤继续验证或修正。

这很像我们培养做题习惯：普通模型更像习惯于快思考的人，扫一眼就交卷；而循环训练出来的模型更像在训练阶段就被强迫慢思考，每次都要推敲几轮才允许停。部署时你再给它多想一会儿的预算，它才知道如何把这些预算用在刀刃上，而不是把额外计算浪费在无意义的重复上。

顺带一提，论文里还有一个很有意思的观察：在实验里，循环步数超过训练深度后，某些 benchmark 可能会退化，但安全对齐反而会随着循环步数增加而改善，甚至包括外推步数。这至少说明一件事：当多想几轮变得可控，推理能力之外，安全机制也可能被重新设计。

## 三、Rubin：更快的反应，更大的工作记忆

说完 Ouro 对脑回路的升级，我们再回到现实：多想几轮意味着更多计算、更多中间状态、更多 KV cache、更多数据搬运。你可以有再聪明的思考方式，但如果每一步都卡在内存和速度瓶颈上，最后还是会被拖回成本地狱。

这就是我看 NVIDIA Rubin 时最强烈的感受：它整套系统是在把推理时扩展从方法论拉进工业化。黄仁勋用让 AI 遇到问题时多想一会儿做铺垫，随后把 Rubin 描述成一套协同设计的下一代 AI 计算平台：Vera CPU、Rubin GPU、NVLink 6、ConnectX-9、BlueField-4、Spectrum-6 一起上，目标是把推理成本打下来，让多想变得经济可行。

但这里有个容易被误解的点：很多人第一反应会把 Rubin 的上下文存储平台类比成模型获得了长期记忆。我觉得更准确的说法是：Rubin 重做的是工作记忆（working memory），不是把知识塞进参数里变成长期记忆，也不是用户调用时临时注入的外置记忆。

把记忆分层会更清楚：模型权重里的知识是长期记忆；KV cache、推理中间状态是工作记忆；RAG/日志/向量库是外置记忆。过去的大模型在多轮对话、复杂工具调用、多智能体协作里变慢，本质上经常是工作记忆的放置与流动出了问题：要么挤在昂贵且容量有限的 GPU 显存里，要么掉到慢存储里拖死吞吐。Rubin 这次披露的推理上下文内存存储平台想做的，就是在 GPU 内存和传统存储之间建立第三层：容量更大、速度更快、还能跨节点共享，让长上下文和多智能体协作不再越跑越慢。

可以把它想象成：以前我们让一个没有工作台的人破案，每次都要把录像翻一遍、笔记抄一遍；现在给他配了一个更大的桌面、更近的资料柜、更快的传送带，还能和其他侦探共享线索。侦探是不是更聪明了？未必。但他能更稳定、更便宜地多想几轮，而且不会因为资料搬运而崩掉。

## 四、两类预算的分配，决定推理时代的竞争力

当我把循环模型和 Rubin 放在一起看时，一个画面变得很清晰：AI 正在从更大走向更会分配预算。而这个预算至少分成两类。

第一类是思考预算（compute budget）。循环模型把多想几轮内建进结构，并提供了类似 Q-exit 这种部署时可调的阈值，让同一个模型在不同预算下跑出不同的效果。

第二类是记忆预算（memory budget）。循环推理天然会带来 KV cache 的倍增压力，论文里也直接承认朴素做法会造成 4× 内存开销，然后给出一个很工程、也很现实的解决思路：在 decoding 阶段做 KV cache reuse，用 last-step 或 averaged 的共享策略，把内存打回 1/4，而且几乎不掉性能。硬件侧的 Rubin 则更进一步，把上下文放置与共享做成系统级第三层记忆，让大规模推理和多智能体协作的工作记忆不再成为瓶颈。

所以如果让我用一句话总结：循环模型在升级大脑的思维回路，Rubin 在升级大脑的工作台与记忆系统。它们合起来，正在把 Test-time Scaling 从概念变成生产力。

## 五、一些值得继续关注的点

- 循环（或更广义的迭代推理）能不能打败目前的训练和调用方式，成为更优路径？这取决于论文里那些数据的普适性是否足够好。

- Memory 这个赛道不仅拥挤，而且已经开始分层了：从最底层的硬件到最上层的用户主权，每一层都值得关注。

- 开个脑洞：安全会不会也变成一个可分配的预算？循环步数增加可能改善安全对齐的观察很耐人寻味。如果多想一会儿不仅能更聪明，还能更谨慎，那安全与性能的关系可能会被重写。
